\chapter{Implementation}
In this chapter we will go through how the original version of REST as well as our variants were implemented. Without access to the source code we had to make assumptions for parts of the algorithm, in addition some smaller parts were changed from the original. The code for this algorithm was written in rust and the source code is available on "github.com". The chapter is divided into two sections, detailing the two different parts of the algorithm namely, reference set construction and the compression algorithm.

\section{Reference Set Construction}
\subsection{Original REST implementation}
For the reference set constructino we implement the strategy CA-5, which stands for compression based approach with 5 as a compression ratio thresold. This method uses the attempts uses the compression ratio of of trajectories new to the set to determine wether they are redundant to the set. If a trajectory is compressed with a ratio larger than the threshold it is considered redundant, i.e. the set already covers the trajectories to a sufficient degree. CA-5 begins with an empty reference set and then iterates over a subset of all trajectories. For each iteration it determines wether the trajectory is redundant or not. Non redundant trajectories are added to the reference set, for redundant trajectories nothing is done and the iterations simply continue. See code listing \ref{lst:ca_og} for the pseudo code of reference set construction.

\input{pseudo/compression-based_og.tex}

The reference set grows as trajectories from the subset are added, it will have high coverage for the trajectories in the subset, because each trajectory is either in the set or can be represented by the set. The coverage for all trajectories depends on the size of the input subset. %More on size? And maybe another RQ

The authors of REST used a hashset as the datastructure for the reference trajectories, we chose to use an array instead because the uniqueness of the trajectories in the reference set is not a concern. Trajectories will only be added if they are non-redundant for the current reference set, therefore there is no need to check uniqueness using a hashset. We therefore used an array because it has much cheaper insertions.

One nuance of the algorithm which should be discussed is that the entire trajectory is added to the reference set. A trajectory as a whole can be determined non-redundant, even if some subtrajectories are redundant. The algorithm could then add only the non-redundant parts of the trajectory. This would not be an expensive process as the trajectory has already been compressed and divided into references and points. However, from our interpretation of \cite{zhao2018rest} section 3.3 the trajectory in its entirety is added to the reference set. Therefore we decided to add the the entire trajectory to the set to stay close to the original version of REST in order to answer RQ1 and RQ2.

\subsection{Spatial Filter Variant}
\input{pseudo/compression-based_sf.tex}
The compression algorithm uses the reference set as a basis for compressing new trajectories. Searching through the entire set for each compression to find candidates to use as references is highly inefficient. Therefore we implement a spatial filter so that the compression algorithm only consideres reference trajectories that are close to the trajectory currently being compressed. The spatial filter is an R-tree containing each point of each trajectory in the reference set. A leaf node in the R-tree contains the position of the point and a tuple index \textit{(i, j)}, \textit{i} is the index of the point's parent trajectory and \textit{j} is the index to the point itself. The psuedo code for reference set construction with a spatial filter is shown in code listing \ref{lst:ca_sf}

\section{Compression Algorithm}
\subsection{Greedy Spatial Compression}
For the compression algorithm we implemented Greedy Spatial Compression (GSC) from REST. This is an algorithm that searches greedily for the best result. It attempts to find a matching reference trajectory for the longest possible subtrajectory starting in \textit{t[0]}. If no MRT is found then it stores the t[0]-t[1] raw and tries searches for an MRT for the longest subtrajectory starting in \textit{t[1]}. After this it picks up at the end of the subtrajectory. The core function searching for MRTs is written as pseudo code in \cite{zhao2018rest} algorthm 1. We have implemented this conceptually the same way, but with some pruning of branches and practical programming considerations.

In our version of greedy mrt expand it iterates through each \textit{candidate trajectory} and returns the the MRT of the longest subtrajectory. One clarification: the goal is not a long MRT, but an MRT for a long ST. By compressing with MRTs from long STs we need fewer references to compress T in its entirety. The candidate trajectories depend on the spatial filter. Without a spatial filter the candidate trajectories are simply all trajectories in the reference set. With a spatial filter the candidate trajectories are all subtrajectories of reference trajectories that start within a certain spatial filter distance of \textit{t[0]}. This is why each point of the reference trajectory is mapped in the r-tree when the reference set is built. After the longest ST with an MRT from the set is collected in each iteration of these, the MRT of the longest ST is again returned (this time global). And now the greedy mrt expand finishes. To ensure compression the whole T, a wrapper is placed around greedy mrt expand, which stores the sequence of MRTs and raw points globally. The code for greedy mrt expand is shown in CODE LISTING 3.
\subsection{Trajectory Similarity}