\chapter{Methodology}
\label{chap:method}
% What I did to answer the research questions

% - Prepare a dataset of trajectories to compress
% - Create an environment where you can compare a similar amount of trajectories on the same machine
% - Implement the REST algorithm in rust
% - Implement MaxDTW in rust
% - Implement DP line simplification in rust
% - Wait

This chapter discusses the methodology used to address the research questions presented in section \ref{sec:questions}. The methodology involved preparing a dataset of trajectories for compression and implementing various compression algorithms. The algorithms were then compared based on their compression ratio and runtime. We implemented multiple variations of the REST algorithm and a Douglas-Peucker variant with a Max DTW halting condition. This was necessary to ensure a fair comparison with REST, as DP uses perpendicular distance as the halting condition.

In this thesis our focus is on spatial compression. Any temporal data is either removed or not taken into consideration, and the compression algorithms solely deal with spatial data.

\section{Dataset}
This section discusses the dataset used for the experiments and the preprocessing steps. The dataset consists of taxi trips that occurred in the city of Porto over the course of a year. The dataset is gathered from \textcite{porto}. Each taxi trip is represented as a separate trajectory in the dataset. Trajectories are made up of polylines of points with latitudes and longitudes, along with some metadata. The points were logged at 15-second intervals using mobile terminals in the taxis. In total, there are 1.7 million trajectories in the dataset. The separation of trips into individual trajectories is a key feature of this dataset. The Porto dataset only records movements when the taxi is active, traveling from point A to point B, unlike other datasets that include location data for the entire day. The type of trajectories generated from a trip is more interesting than the downtime between trips. During downtime, the taxis typically hover in frequently traveled areas or drive to the next pickup location. These types of trajectories are less interesting because they do not always represent the fastest path and are more challenging to interpret.

To prepare the data for spatial compression, the polylines were written to a CSV file, and the metadata was discarded. Additionally, any polylines with a length of less than two points were removed, as this data is invalid, and the compression algorithms require at least two points in each trajectory. Through analysis, it was discovered that some trajectories had large jumps in location, indicating invalid data. Although these trajectories do not resemble realistic taxi movement, more time was not spent on data cleaning as the purpose of this thesis is to compare different compression algorithms. It is acceptable for some trajectories to be unrealistic as long as the algorithms are being tested on the same data. However, the potential impact of unrealistic trajectories should be considered if it is reasonable to believe that they may affect the algorithms differently.

\section{REST}
This section describes how the REST algorithm was developed and discusses the variants created to speed up the original algorithm. It is important to note that these variants can be used in combination with one another.

To clarify regarding the terminology, in section \ref{sec:REST}, REST is a collection of reference based and data driven techniques for trajectory compression. However, when referring to REST from this point forward, we are specifically referring to our own implementation. Our implementation consists of the compression-based approach (CA) for constructing the reference set, and the greedy spatial compression (GSC) for trajectory compression. We chose CA since it showed the best results in the experiments conducted by \textcite{zhao2018rest}. GSC was selected over optimal spatial compression (OSC), because it strikes a balance between runtime efficiency and compression ratio.

In our efforts to speed up REST, we focused on reducing the number of DTW calculations rather than reducing the cost of each calculation. This involved changing the way REST compresses in order to reduce the amount of trajectory comparisons, not reducing the cost of each comparison. Various techniques from chapter \ref{chap:bg} have been used and the applications of them will be explained in the relevant sections.

The source code for REST was not made available from \textcite{zhao2018rest}. Nonetheless, we contacted the authors, requesting to access it, but received no reply. Without the source code, the algorithm had to be recreated using the descriptions of algorithms and pseudocode from \textcite{zhao2018rest}. This was time-consuming and some implementation details were not entirely clear from the text. Consequently, some assumptions had to be made. The most relevant assumptions will be discussed, others have been made using experimentation (selecting the best), common sense or theory. Regardless, we are confident our implementation captures the essence of CA and GSC.

\subsection{Reference Set Construction}
This section will further discuss the motivation behind the compression-based approach, as well as our method of implementation. An efficient reference set has high \textit{coverage} with low redundancy, \textit{coverage} refers to the extent which the reference set represents the entire set of trajectories. A set with high coverage would yield a high compression ratio for all trajectories in the set. Redundancy refers to how much overlap there is between trajectories in the set. Striking a balance between coverage and redundancy is important, as too high coverage will likely lead to many trajectories in the set, leading to high redundancy. Conversely, tolerating too little redundancy will lead to too few trajectories in the set, causing insufficient coverage.
%The relationship between coverage and redundancy strongly correlate to precision and recall in information retrieval. Low redundancy equals high precision and high coverage equals high recall.

The compression-based approach naturally minimizes redundancy using the threshold and achieves sufficient coverage when using a large sample. It compresses a sample of all trajectories and adds those with compression ratios below a threshold to the set, indicating they are not well represented. The compression ratio threshold affects the redundancy tolerance; a lower threshold indicates a higher tolerance.

The pseudocode for this algorithm can be seen in code listing \ref{lst:ca_og}. The set is initialized as an empty list and trajectories are compressed and added or discarded iteratively. With a sufficient sample size the reference set will have high coverage for the entire set of trajectories.
\input{pseudo/compression-based_og.tex}

\subsection{Compression Algorithm}
For the trajectory compression we implemented Greedy Spatial Compression (GSC) from REST. This is an algorithm that compresses trajectories using a reference set. It compresses by converting trajectories into sequences of references and "direct points", known as the compression sequence. The references are pointers to subtrajectories in the reference set that match some part of the trajectory. The reference is considered the compression of the part of the trajectory it represents. They are called \textit{Matchable Reference Trajectories} and the definition is taken directly from \textcite{zhao2018rest}.
\begin{quote}
    \begin{definition}
        \label{def:mrt}
        Matchable Reference Trajectory (MRT). Given a sub-trajectory T(i, j) and a spatial deviation threshold $\epsilon_{s}$, its matchable reference trajectory set, denoted as M(T(i, j)), includes all the reference sub-trajectories with less-than-$\epsilon_{s}$ MaxDTW distance with T(i, j), i.e.,
    \end{definition}
    \begin{equation}\label{eq:mrt}
        \begin{aligned}
            M(T(i,j)) = \{ \mathbb{T}(k,g) \mid \mathbb{T} \in \mathcal{R}, i \leq k \leq g \leq \left\lvert \mathbb{T} \right\rvert, \\
            \text{MaxDTW}(T(i,j), \mathbb{T}(k,g)) \leq \epsilon_{s} \}
        \end{aligned}
    \end{equation}
\end{quote}

From this and the properties of MaxDTW, \textcite{zhao2018rest} also derived a rule.

\begin{quote}
    \begin{lemma}\label{lemma:1}
        Any sub-trajectory of the MRT of T(i, j) is also an MRT of sub-trajectory of T(i, j).q
    \end{lemma}
\end{quote}


With the definition of an MRT and Lemma \ref{lemma:1}, the GSC algorithm can be further explained. To compress a trajectory $T = [t_0, t_1, ..., t_n]$ GSC, searches for MRTs for $T$. The subtrajectories of $T$ with no MRTs are not compressed, but stored directly in the compression sequence. The amount of subtrajectories with no MRTs depends on the coverage of the reference set. GSC's goal is to represent $T$ with as few MRTs as possible. Consequently, it attempts to use MRTs for as long subtrajectories as possible.

However, GSC searches for the matches greedily. Greedy in the sense that it selects the longest subtrajectory with an MRT originating from the first point. GSC starts by finding MRTs for the subtrajectory $[t_0, t_1]$, and uses these to find MRTs for $[t_0, t_1, t_2]$. In contrast, an optimal solution would also consider using MRTs for subtrajectories originating in a point other than $t_0$, as it might lead to a more efficient compression.

The is search method is called \textit{greedy\_mrt\_expand} and the source code for this can be seen in code listing \ref{lst:ca_expand}. The function's input and output is shown in line 1-6, it has 4 input parameters:

\input{pseudo/greedy_mrt_expand.tex}

\begin{itemize}
    \item{\textit{t} - The trajectory being compressed}
    \item{\textit{candidate\_reference\_trajectories} - The reference trajectories used in compression}
    \item{\textit{spatial\_deviation} - The spatial deviation threshold for MRTs}
\end{itemize}

The output is a tuple (\textit{m}, $r_{0,m}$) or None. Where \textit{m} is the last index of the subtrajectory corresponding to the MRT and $r_{0,m}$ is the MRT itself. The compressed subtrajectory is given by [$t_0$, ..., $t_m$], the first index is always 0 because of the greedy search strategy. The compressed subtrajectory is the longest subtrajectory with an MRT from the \textit{candidate\_reference\_trajectories}. The input \textit{candidate\_reference\_trajectories} is either the entire reference set, or a subset of the reference set created using a spatial filter, this is discussed in section \ref{sec:sf}. None is returned if no MRTs were found.

Line 7 in code listing \ref{lst:ca_expand} is the start of the function and initializes a map for (subtrajectory, MRT) tuples. Line 8-47 is the code block for search in each reference trajectory \textit{rt}. It initializes \textit{current\_mrts} with all length two MRTs for [$t_0$, ..., $t_i$] where $i = 1$ (line 9-15). It does this by calculating the Max DTW distance between [$t_0$, $t_1$] and subtrajectories [$rt_j$, $rt_{j+1}$] for $j = 0, 1, ..., m-2$, where $m$ is the length of the $rt$. The variable \textit{current\_mrts} is the collection of all MRTs for [$t_0$, $t_1$]. It follows from lemma \ref{lemma:1} that this will be the basis for all MRTs. This is because the Max DTW distance can never decrease for a longer subtrajectory.

The loop in line 17-47 uses \textit{current\_mrts} to find MRTs for $st_{i+1}$ = [$t_0$, ..., $t_{i+1}$] and stores MRTs to the global set (line 20-26). For each $rt$ = [$rt_0$, ..., $rt_m$] in \textit{current\_mrts} (matches for $st_i$), three expansions are tested for a match with $st_{i+1}$:
\begin{itemize}
    \item {[$rt_0$, ..., $rt_m$]}
    \item {[$rt_m$, ..., $rt_{m+1}$]}
    \item {[$rt_0$, ..., $rt_{m+1}$]}
\end{itemize}
This aligns with the expansion in the original REST MRT search algorithm, from \textcite{zhao2018rest}.

The loop continues, increasing $i$ for each iteration, as long as an MRT can be found for $st_{i+1}$ or until $i + 1 = |st|$. In addition, an arbitrary match from the \textit{current\_mrts} is added to the global map if no entry exists for $st_{i}$, for each iteration. When this loop finishes, the algorithm repeats the process for the next reference trajectory. In the end, on line 49 the (subtrajectory\_index, MRT) tuple for the longest subtrajectory from the global map is returned.

% Regarding MRT selection when multiple are available, this was done arbitrarily. From the definition, an MRT has a DTW distance to the subtrajectory being compressed lower than the spatial deviation threshold. This means one reference trajectory can have a lower DTW distance than another, while both are considered MRTs. From this one could argue that the algorithm should select the MRT with the lowest distance. However, this implementation of REST applies bounded lossy compression, which means any result within the threshold is valid or "good enough". Therefore, no resources are spent locating the best MRT within the threshold, even though the resources required aren't large because the DTW distance is calculated anyway. If this were implemented, it could be considered "best effort bounded lossy compression". Comparing that to bounded lossy compression would be interesting, but it was not done due to time constrains. It is also likely that it wouldn't make a large difference because a reference trajectory with a significantly lower DTW distance could also match for a longer subtrajectory. This means that if there were multiple MRTs to select, they are likely close in DTW distance to the relevant subtrajectory. The way the reference trajectories expand and how this effects the DTW distance is further discussed in results and discussion. %Reference definition of spatial greedy compression
%Nuke last part about best effort lossy compression
%Maybe something about arbitrary match from the definition of bounded lossy compression

To compress a whole trajectory, a wrapper function uses greedy\_mrt\_expand iteratively. For instance, say that a trajectory $U = [u_0, ..., u_n]$, where $n = 50$, is being compressed. The first iteration of greedy\_mrt\_expand returns (25, $MRT_1$). Now the wrapper adds $MRT_1$ to the compression sequence, and sends $[u_{25} ..., u_n]$ into greedy\_mrt\_expand. The process continues until all points up to $u_n$ are compressed. An example compression sequence could be, $[MRT_1, u_{25}, u_{26}, MRT_2]$. In this case $[u_{25}, u_{26}]$ had no MRT and was stored directly in the compression sequence.

\subsection{Exclusive REST}
One nuance of the reference set construction algorithm which should be discussed is that the entire trajectory is added to the reference set. A trajectory as a whole can be determined non-redundant, even if some subtrajectories are redundant. The algorithm could then add only the non-redundant parts of the trajectory. This would not be an expensive process as the trajectory has already been compressed and divided into references and points. However, from our interpretation of \cite{zhao2018rest} section 3.3 the trajectory in its entirety is added to the reference set. This impacts the reference set by adding more redundancy to the set. Meanwhile, it also leads to more coverage since more of the trajectories are added. Whereas an implementation which only adds the non-redundant parts would have less redundancy and lower coverage.

In order to analyze the difference in these approaches both were implemented. The version adding the entire trajectory to the set is called REST, since this is interpreted as being the original implementation. While the other version only adding non-redundant parts of the trajectory to the set is called Exclusive REST (XREST).

Another key difference, is that a reference set constructed with XREST will have shorter trajectories than a reference set constructed with REST (because REST adds whole trajectories). Shorter reference trajectories might cause shorter matches in compression. Consequently, the compression will require more references to complete. Following this intuition it seems like longer reference trajectories are favorable to shorter ones. In extension, this suggests that REST will outperform XREST in terms of compression ratio.

\subsection{Spatial Filter}
\label{sec:sf}
% Set construction
The compression algorithm uses the reference set as a basis for compressing new trajectories. Searching through the entire set for each compression to find candidates to use as references is inefficient. Therefore, we implement a spatial filter using an R-tree, so the compression algorithm only considers reference trajectories that are close to the trajectory being compressed. To achieve this, all trajectories added to the reference set must also be mapped in the R-tree. This change to the reference set construction can be seen in code listing \ref{lst:ca_sf}. Leaf nodes in the R-tree contains the point in latitude longitude and a metadata tuple index \textit{(i, j)}, where \textit{i} is the index of the point's parent trajectory and \textit{j} is the index within the parent trajectory. Points are added to the R-tree iteratively as trajectories are added to the reference set.

This R-tree is used with a window query to find points and their corresponding reference trajectories in proximity of the desired area. The window query is used to generate the \textit{candidate\_reference\_trajectories} for the compression algorithm. Without a spatial filter, this is simply all trajectories in the reference set.

The window query uses the first point of trajectory being compressed as the center of the window. This can be viewed as creating a box around the first point and selecting all points within the box. The box in our implementation is a square with the length of the sides as an input parameter, called the \textit{window size}. A larger square means more points will be included, which leads to longer runtimes, but a more thorough search. While a smaller square will cover less area in a shorter amount of time.

After the reference set is built from a sample of all trajectories, the data will not change and can be considered static. A packed R-tree variant would likely perform well compared to a dynamic R-tree after this stage. The Packed Hilbert R-tree is simple and efficient for low dimensionality. Additionally, it has a very high storage utilization, which is beneficial if the size of the reference set is a concern. However, due to time constraints, this was not implemented. Nonetheless, the authors recommend exploring packing the R-tree after reference set construction as future work.

Note that REST without a spatial filter can be seen as using a window size that approaches infinity, because an infinitely large window will include all possible points. Using all possible points will produce the best possible compression ratio in the longest possible amount of time, compared to using spatial filter. Therefore, the runtime and compression ratio of REST can be considered the upper bound for REST with a spatial filter. Selecting a good window size should balance a good compression ratio with a low runtime. Analysis of different sizes will be explored and further discussed in chapter \ref{chap:res}.

% Note that the compression algorithm is used to generate the reference set. Changes in the compression algorithm will therefore affect the qualities of the reference set. Sort of indirectly. AS WELL AS changing the compression that takes place later, deconstructing which change happens where, how, is a complicated process. And this is why I can write 100+pgs of discussion.

\input{pseudo/compression-based_sf.tex}
\subsection{K-Nearest Neighbors}
This section describes a change, inspired by the K-Nearest Neighbors (KNN) algorithm, applied to \textit{greedy\_mrt\_expand}. From the definition of an MRT, definition \ref{def:mrt}, we have that any match with a distance below the threshold is valid. The definition accepts a wide range of matches with no distinction between a perfect match and an almost invalid match. This change, suggests ranking matches by MaxDTW distance and selecting the K best matches (or K-Nearest Neighbors) for further expansion, where K is an input parameter. As a result, the amount of expansions to consider is reduced, which leads to performance gains and some reduction of compression ratio as some optimal matches will be discarded.

Comparing KNN to the Spatial Filter described in section \ref{sec:sf}, KNN can be seen as a fine grain filter, with the spatial filter as a more coarse filter (depending on the window size). We believe that the combination of both filters will be effective, using the spatial filter with a large window size to select good candidate trajectories and using KNN later to select the best of these matches.

\section{Max DTW}
\label{sec:dtw_impl}
Max DTW has been implemented in Rust following the definitions in \ref{sec:dtw}, the source code for the main function can be seen in code listing \ref{lst:maxdtw}. One important efficiency gain was through memoization of DTW solutions, which is represented by the \textit{map} parameter. \\ The \textit{greedy\_mrt\_expand} algorithm in code listing \ref{lst:ca_expand}, expands with one point at a time, this is equivalent to adding one row and column to the DTW matrix. Without memoization, the entire matrix must be recalculated. However, with memoization, only the new column and row need to be calculated, using the previously computed accumulated cost matrix. This lead to a significant performance gain for the distance measure.

Additionally, an optional Sakoe-Chiba band was implemented like described in \ref{sec:sakoe}. This was included to speed up DTW, however it is the only speed-up method which directly changes how DTW is calculated, the other methods reduce the amount of DTW calculations. The Sakoe-Chiba band only estimates the real Max DTW value. Errors in the distance can lead to reference trajectories breaking the MaxDTW threshold condition. Therefore, using the band should be considered carefully. Analyzing the error introduced by different the band sizes is left as future work.

\input{pseudo/maxdtw.tex}

Kommentar: Kan gå inn på implementasjon, men det er mye gjenfortelling fra bakgrunn.

\section{Douglas-Peucker with MaxDTW}
DP using the PD distance measure from section \ref{subsec:PD} as the halting condition, is simpler than a Max DTW threshold. The compression process is much cheaper than DTW, but it does not meet the same criteria, making the results incomparable. However, they are quite similar, the difference is that DP calculates the perpendicular distance to the line connecting the previous and subsequent point in the line, while Max DTW calculates the longest distance of all the optimal point alignments. The limitations of DP are that the alignment of point to line is not as robust as the DTW alignment. Additionally, the perpendicular distance to the line of previous and subsequent point is a much less strict requirement than point to point distance.

In order to compare REST with another compression strategy, DP-Max DTW was created. It uses the approach of DP, but the halting condition is a Max DTW threshold. The line simplification continues until the Max DTW distance between the simplified line and the original line is below the spatial deviation threshold. This way the spatial deviation parameter can be applied to REST and DP-Max DTW, guaranteeing the same quality of the compression. Only then can runtime and compression ratio can be compared for the different strategies.

% Bounded lossy compression
\section{Experimental setup}
In this section the experimental setup to answer the research questions is discussed. The questions require a measure of runtime and compression ratio for different algorithms. All experiments must be conducted on the same machine and with the same data. Therefore, an environment which loaded data an executed code based on input parameters was set up on a virtual machine.

\subsection{Runtime}
The runtime was measured from the beginning of the algorithm execution until the end. For REST, both reference set construction and the trajectory compression was included in the runtime.
\subsection{Compression Ratio}
In order to calculate compression ratio for REST we must track how many points are stored directly and how many are stored by reference, and keep track of the points in the reference set. Additionally, the sizes of the different element must be determined.

Each coordinate of a point is stored as the Rust type \textit{i32} (fixed-point), which uses 4 bytes. A point with both latitude and longitude takes 8 bytes in total to store. A reference to a slice in rust consists of the pointer to the first element and the length of the slice. The pointer and the length are both 8 bytes, totaling 16 bytes for the whole reference.

The compression ratio is split into two, one including the reference set and one without. This can be useful for analyzing how the reference set size affects the algorithm. The \textit{average compression ratio} is defined by the following equation.

\begin{equation}\label{eq:avg_cr}
    avg\_cr = \frac{all\_points \times 8}{references \times 16 + direct\_points \times 8}
\end{equation}
The \textit{total compression ratio} is similar, but includes the points in the reference set.
\begin{equation}\label{eq:tot_cr}
    tot\_cr = \frac{(all\_points + points\_in\_set) \times 8}{references \times 16 + direct\_points \times 8}
\end{equation}
% At one point, we considered using the external Rust library \textit{dtw\_rs} instead of a custom implementation. To prepare for this, we contributed to it by finishing the Sakoe-Chiba band implementation. However, due to the aforementioned memoization, which requires some state awareness from the local environment, integrating this library demanded too many modiciations. Consequently, we decided to develop our own implementation. Although this contribution was not used in our final results, it is available here: \\ \href{https://github.com/shshemi/dtw-rs/pull/1/files}{https://github.com/shshemi/dtw-rs/pull/1/files}.
% \section{Experiment setup}

