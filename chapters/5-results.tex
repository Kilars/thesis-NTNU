\chapter{Results}
\label{chap:res}
In this section we will present and discuss the results from the experiments on the REST implementation. The experiments were conducted on an Intel(R) Xeon(R) Gold 6426Y processor with 500 GB RAM. Samples from the Porto dataset are used as data.

For these experiments there are many parameters that can be adjusted. To keep this section concise we will only analyze the parameters that are most relevant to our research questions. Other parameters are set using the experimental results from \textcite{zhao2018rest}. For example, the spatial deviation threshold for an MRT, definition \ref{def:mrt}, is set to $\epsilon_s = 200 m$, because it produced the best results in the previous experiments. The \textit{Compression Threshold} for the reference set construction as shown in code listing \ref{lst:ca_og} is set to $CT = 3$. In the experiments from \textcite{zhao2018rest}, both $CT =3$ and $CT = 5$ were tested. The latter showed slightly better results, but in our experience, it was slower. Therefore, we set $CT = 3$ despite slightly worse performance, due to time constraints. Additionally, as long as all variants use the same parameter, the comparisons will be valid and likely hold true for both thresholds.

To describe different variants of REST, labels for each variant have been created. The spatial filter variant is denoted by the suffix \textit{-SFX}, where \textit{X} is the window size in meters. The Sakoe-Chiba band is denoted by the suffix \textit{-BNDX}, where \textit{X} is the band size. The KNN variant is denoted by the suffix \textit{-KNNX}, where \textit{X} is the number of MRTs selected. An example label can be \textit{REST-SF75-KNN3-BND20}, this represents REST using a spatial filter with window size $75\times75 m^2$, selecting the 3 best MRTs, and applying a Sakoe-Chiba band of size 20 to each Max DTW calculation.

\section{Compression Ratio and Runtime}
In order to answer RQ1 REST and the variants have been measured when compressing trajectories. These experiments have been executed by first building a reference set with a sample size of 10K, to then use the reference set during compression. The trajectories in the sample used to build the reference set were excluded from the later compression, since they would have a much higher compression ratio than other trajectories. Additionally, in order to answer RQ2, DP-DTW has been measured when compressing the same amount of trajectories, using the same spatial deviation threshold.

\begin{figure}[h]
    \begin{minipage}{0.99\linewidth}
        \centering
        \includegraphics[height=7cm, keepaspectratio]{../code/plotting/tot_compression.png}
        \caption{Total Compression Ratio over trajectories compressed. The reference set was constructed using a sample size of 10K.}
        \label{fig:n_compression}
    \end{minipage}
\end{figure}

In figure \ref{fig:n_compression} it is clear that REST has the best compression ratio. It has a total compression ratio of $5.11$ for 60K compressed trajectories. Note that REST and XREST only compressed 60K trajectories instead of 100K due to time constraints, in both figure \ref{fig:n_compression} and figure \ref{fig:n_runtime}. The spatial filter variants have a lower compression ratio than REST, but it increases with a larger window size. This is expected because REST can be seen as having an infinitely large window size. In regard to RQ1 this shows that the spatial filter variant with a sufficiently large window size can reach close to the same compression ratio as REST. The KNN variants in combination with spatial filter shows promising results as they only slightly reduce the compression ratio compared to spatial filter with no KNN. \textit{REST-SF120-KNN3} has a total compression ratio at $4.86$ for 60K compressed trajectories, the corresponding number for \textit{REST} is $5.09$. This shows that \textit{REST-SF120-KNN3} has 95\% \textit{REST}'s compression ratio.

KNN used directly on REST did not show promising results and is not shown in the results. This is likely because REST uses all reference trajectories to search for MRTs, selecting a handful early in the process seems to miss out on better suggestions.

In general the total compression grows as more trajectories are compressed. This is because the number of compressed trajectories grows larger in relation to the set size. From equation \ref{eq:tot_cr}, this represents \textit{points\_in\_set} becoming smaller compared to \textit{all\_points}. In the case where the amount of compressed trajectories keeps growing, the size of the reference set would eventually become negligible, causing the total compression to approach the average compression. The average compression ratio (not shown), stays near the same level for all variants, around $7.4$ for REST. Moreover, the amount of compressions needed to make the reference set negligible is much larger than any realistic use case. The result in our experiments is that the total compression grows slower and slower.

XREST has a much lower compression ratio than the REST and REST based variants. This indicates that the reference set of XREST has low coverage. This is somewhat expected as the exclusive strategy of XREST requires a larger sample size to achieve the same coverage as REST. Additionally, this can be due to differences in the length of the reference trajectories. Shorter reference trajectories lead to a longer as less efficient compression sequences. Another difference between REST and XREST is the slower growth of compression ratio for XREST, this is because XREST has a smaller reference set. Thereby, noticing less of a change as the number of trajectories compressed increase.

DP-DTW has a low compression ratio of 1.43, more than 3 times lower than REST. This suggests that a line simplification algorithm can not achieve the same compression ratio as a reference based strategy. In regard to RQ2 this is a strong indication that reference based strategy is superior to DP in terms of compression ratio. DP only considers one trajectory at a time and does not benefit from any similarities between trajectories in the dataset as a whole. REST uses the similarities for a much more effective compression. Additionally, in the experiments of \textit{zhao2018rest}, they compared DP with SED as the accuracy metric to REST, these results also showed a much lower compression ratio using line simplification.

\begin{figure}[h]
    \begin{minipage}{0.99\linewidth}
        \centering
        \includegraphics[height=7cm, keepaspectratio]{../code/plotting/n_runtime.png}
        \caption{Runtime over trajectories compressed. The reference set construction is included in the time, using a 10K sample size.}
        \label{fig:n_runtime}
    \end{minipage}
\end{figure}

In figure \ref{fig:n_runtime} we see that REST and XREST are significantly slower than other variants. This demonstrates the inefficiency of using the entire reference set as candidates in MRT search. XREST is slightly faster than REST, this is because the reference set is smaller, leading to less reference trajectories to search through. Although, with a smaller reference set we would expect XREST to be even faster. This indicates that the quality of the reference set is slowing it. XREST has shorter trajectories in the reference set, causing a more segmented compression sequence.

The spatial filter variants are significantly faster than REST and XREST. The spatial filter efficiently selects a subset of the reference set to use for MRT search. Note that \textit{REST-SF70} has approximately the same runtime \textit{REST-SF120}, this is unexpected as a smaller window size is expected to find candidate reference trajectories, and hence do fewer trajectory comparisons. Nonetheless, the smaller window size is equally fast. Presumably, this is because the reference set is bigger. When using a smaller window size, fewer trajectories are found in reference set construction as well, leading to a larger set. With a larger set, there are more trajectories comparisons even within the smaller window size, leading to a slower runtime.

We see that the KNN variants in combination with a spatial filter leads to a faster runtime. Interestingly, Also note that \textit{XREST-SF120} is 35 times slower than \textit{XREST-SF120-KNN3}. This shows that KNN had a good impact when using XREST with a spatial filter to reduce runtime. It also had a good impact for REST, but not as large as with XREST. \textit{REST-SF120-KNN3} compresses the same amount of trajectories 9 times faster than \textit{REST}. Considering RQ2, this shows a significant speed-up of the original REST algorithm.

DP-DTW is the slowest of all the compression algorithms, 1.4 times slower than \textit{REST} and 13 times slower than \textit{REST-SF120-KNN3}. This is because it performs many DTW comparisons. DP-DTW calculates the Max DTW distance between the simplified trajectory and the complete trajectory for each iteration. The complete trajectory has the full length of the original trajectory, while the simplified trajectory begins with length two and grows from there. Because DTW is $O(N \times M)$, where $N$ and $M$ is the length of the trajectories, and $N$ is always the full trajectory length, this will be an expensive process. In terms of RQ2, we see that REST slightly outperforms DP-DTW in terms of runtime and the REST variation using a spatial filter with KNN drastically outperforms it.

All in all, looking at both compression ratio and runtime, we see that the spatial filter variants are much faster than REST and XREST, while reaching high compression ratios. In regard to RQ1, the best performing variant \textit{REST-SF120-KNN3}, achieved 95\% of \textit{REST}'s compression ratio 9 times as fast. In terms of achieving similar compression ratios, this is clearly the case for this variant. Although the increase in runtime can be considered significant, it also reflects some limitations for using REST. All REST variants are limited by the speed of the Max DTW calculation, this is the bottleneck for runtime. Speed-ups larger than what is achieved by \textit{REST-SF120-KNN3}, will likely require a cheaper trajectory comparison. The benefit of using DTW is that it is high quality similarity comparison.

Considering that Max DTW is the most resource demanding part of the algorithm, it would be interesting to test REST using another trajectory distance measure. These variants will likely improve the performance of these as well, because trajectory comparison is the main operation.

The DP implementation used as a comparison in the experiments of \textcite{zhao2018rest}, was SED based. This was much faster than the different REST strategies. Therefore, it would be interesting to see how REST would perform using a "Max SED" distance instead of a Max DTW distance for the MRTs spatial deviation threshold. This would likely have a faster runtime.

However, it is important to note that in this case, the quality of compression would not be the same, because SED and Max DTW are different metrics. It is not meaningful to compare them directly. Although, through a practical application comparing a SED based REST and a Max DTW based REST would be interesting. For example through querying the compressed trajectories and comparing the quality of the result, in relation to the achieved compression ratio and runtime efficiency.

\textit{REST} is slow, but achieves high compression ratio by using the entire reference set for each compression. XREST has the lowest compression ratio and the second-highest runtime, however using spatial filter and KNN it is significantly faster than all other variants. This show that the strategy is unable to compete with REST in terms compression ratio, but has the best runtime. The low compression ratio might be due to the sample size being too small, or shorter reference trajectories in the reference set.

A version of XREST with improved compression ratio could compete with REST in both runtime and compression ratio. The strategy seems to speed up significantly when using a spatial filter with KNN, while keeping near the same compression ratio. Nonetheless, the compression ratio is low, there can be multiple reasons for this. Among them are: different qualities of the reference set, too low sample size, or some other difference arising from adding only the non-redundant subtrajectories. An approach mitigating the loss in compression ratio while maintaining the speed would achieve better performance than REST. This could be achieved by only adding non-redundant subtrajectories above a certain length or by using an optimal MRT search.

In order to answer RQ2, we compare DP-DTW to REST and its variants. The results show lower performance in both runtime and compression ratio. The poor compression ratio is most notable, as the runtime is only 1.4 times slower than \textit{REST}. For other variants the difference in runtime is much larger. Moreover, the compression ratio is significantly worse when compared to all variants except for \textit{XREST}.

The Sakoe-Chiba band did not show promising results, it provided a slightly faster runtime, but also reduced compression ratio. The slight performance gain is not worth the loss in quality. That decline in compression ratio using the Sakoe-Chiba band was surprising, we would expect many of the Max DTW matrices to be calculated in a more favorable way, leading to more false positives. A false positive means an MRT is accepted as a match using Sakoe-Chiba, but is in fact invalid using the analytical Max DTW. Since the compression ratio decrease it indicates that there were more false negatives. A false negative occurs if an MRT is valid using the analytical Max DTW, but invalid using the band. In general, the Sakoe-Chiba gives good estimations for calculations where the optimal path is along the main diagonal. This is the case for trajectories that are similar and align well, which we would assume for the best MRT matches. However, it seems that this is not the case. Perhaps variations in point time-frequencies for different trips caused the optimal path to move outside the band, even if trips had a similar trajectory. This would cause the error to produce more false negative compressions.

\section{Sample Size testing}\label{sec:sample_size}
To answer RQ3, different sample sizes were tested in order to analyze the effectiveness of the reference set. These experiments were executed by compressing 100K trajectories using higher sample sizes. This resulted in reference sets with increasing coverage. The effect sample size has on the total compression ratio and the average compression ratio will be discussed in this section. Due to time constrains, results for REST and XREST could not be generated, as increasing the sample size also increases the runtime significantly. However, we believe the sped-up variants will show the same patterns as the REST and XREST.

\begin{figure}[h]
    \begin{minipage}{0.99\linewidth}
        \centering
        \includegraphics[height=7cm, keepaspectratio]{../code/plotting/sample_size_tot.png}
        \caption{Total compression over Sample Size from compressing 100K trajectories.}
        \label{fig:sample_tot}
    \end{minipage}
\end{figure}

In figure \ref{fig:sample_tot}, \textit{REST-SF75-KNN3} has the highest compression ratio for a sample size of 40K, the compression ratio gradually declines as the sample size grows beyond this. As the sample size grows, so does the set size. In order achieve a better compression ratio with a larger set, the compression gain must be large. The slow decline indicates that there is some compression gain, which is outweighed by the increase in set size. The optimal balance between compression and set size is struck using a sample size of 20K, reaching a total compression ratio of $6.44$.

\begin{figure}[h]
    \begin{minipage}{0.99\linewidth}
        \centering
        \includegraphics[height=7cm, keepaspectratio]{../code/plotting/sample_size_avg.png}
        \caption{Average compression over Sample Size from compressing 100K trajectories. Average compression is the average compression ratio of each compressed trajectory, without taking the reference set size into account.}
        \label{fig:sample_avg}
    \end{minipage}
\end{figure}

In figure \ref{fig:sample_avg}, the average compression ratio is shown. The difference between total and average compression is the inclusion or exclusion of the reference set size. The average compression is interesting because it gives insight into the coverage of the reference set. Without taking the reference set size into account this ratio simply shows how well each trajectory is compressed on average. As the sample size increases the average compression ratio also increases. This is expected, because more reference trajectories gives a higher coverage reference set.

However, the growth of the average compression is decreasing, even as the sample size increases. If there is no gain in average compression when sample size increases, this must mean that the coverage of the reference set is approaching 100\%, because no new information is gained from using a larger sample.

The results in figure \ref{fig:sample_avg} are based on compressing 100K trajectories of a total 1.7M in the dataset. Because the compressed trajectories are randomly selected from the dataset, it is very unlikely that the characteristics of the selected subset is significantly different from the rest of the set. Therefore, it is safe to assume that the resulting average compression for N = 100K, would be similar for N = 1M. Since the characteristics of the dataset would be similar for the additional trajectories compressed. Assuming the average compression ratio stays the for a higher N, this would lead to a better total compression.

All in all, the results from varying sample sizes show that a reference set with high coverage can be built using a small sample size relative to the dataset. The average compression grows as coverage increases, while the total compression reaches an optimal point and then declines as the reference set grows in size.